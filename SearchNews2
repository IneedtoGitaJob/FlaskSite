from GoogleNews import GoogleNews
from datetime import date
import urllib.request
import functools
from multiprocessing import Pool
from multiprocessing import cpu_count
import time
from urllib.request import urlopen
import numpy as np
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from bs4 import BeautifulSoup

#Returns a list of urls from the news
def GetNewsUrlsMulti(searchTopic, Years):
    nltk.download("vader_lexicon")
    start_time = time.time()
    current_year = date.today().year
    urlList = []
    result = []

    for x in range(Years):
        for y in range(2):
            googlenews = GoogleNews(start= str(y)+'/01/'+(str)(current_year), end= str(y)+'/28/'+(str)(current_year))
            googlenews.search(searchTopic)
            result.append(''.join(googlenews.get_links()))
            #if result is not None:
        urlList.append((list)(result))
        result = []
        current_year = current_year-1
    print("---GetNewsUrls: %s seconds ---" % (time.time() - start_time))
    return (urlList)

#sets up the array of urls to be multiprocessed
def multiProcessList(y):
    with Pool() as p:
        tempList = list(p.map(CallPositivity, y))
    return(tempList)

#Function called on each array
def CallPositivity(x):
    try:
        return(GetPositivityMulti(x))
    except Exception as error:
        return(-2)

#Entry point for checking webpage positivity
def DisplayPositivity(URL)-> str:
    nltk.download("vader_lexicon")
    x = GetPositivityMulti(URL)
    #No text
    if (x == -1):
        return("black")
    #issue connecting
    if (x == -2):
        return("Fail")
    #increase x by 20% as our HSL is on a scale from 0 to 120 and x is currently scaled 0 to 100
    x = x*1.2
    return ("hsl("+(str)(x)+", 100%, 50%)")

#Returns the positivity of a Web Page
def GetPositivityMulti(URL)-> int:
    sia = SentimentIntensityAnalyzer()
    # Find all text within paragraph tags and find the mean
    sentiment = 0.0
    num = 0
    positivity = -1
    #Set Headers
    req = urllib.request.Request(
        URL,
        data=None,
        headers={
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'
        }
    )
    try:
        html = urlopen(req).read()
    except Exception as error:
        print(error)
        return(-2)

    soup = BeautifulSoup(html, "html.parser")
    for data in soup.find_all("p"):
        # ignore neutral sentences
        compound = sia.polarity_scores(data.get_text())['compound']
        if (compound != 0):
            num = num + 1
            sentiment = sentiment + compound
    if (num != 0):
        if (sentiment >= 0):
            positivity = int((round(50 * (sentiment / num)) + 50))
        else:
            positivity = int((50 - (round(50 * (sentiment / num)) * -1)))
    print("Sentiment:" + str(sentiment/num) +"      Positivity: "+ str(positivity)+ "     URL:" + str(URL))
    return positivity

